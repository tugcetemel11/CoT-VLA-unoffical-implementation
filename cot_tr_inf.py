# -*- coding: utf-8 -*-
"""cot-tr-inf.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ic0EoM0uxU3dXQoceLQWRz-07ua1r3jO
"""

"""
Author: Tuğçe Temel
=========================================
CoT-VLA (Visual Chain-of-Thought) on VILA-U

Unofficial research implementation.
"""

import json
import re
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import AdamW

from vila_u.model.builder import load_pretrained_model
from vila_u.conversation import conv_templates
from vila_u.constants import IMAGE_TOKEN_INDEX
from vila_u.mm_utils import tokenizer_image_token
from vila_u.data import CoTVLADataset

#Configuration

MODEL_PATH = "mit-han-lab/vila-u-7b"
MAX_LEN = 2048
LR = 2e-5
EPOCHS = 1
BATCH_SIZE = 1
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# Load VILA-U


tokenizer, model, image_processor, context_len = load_pretrained_model(
    model_path=MODEL_PATH,
    device_map="auto"
)

tokenizer.add_special_tokens({
    "additional_special_tokens": [
        "<cot>", "</cot>",
        "<action>", "</action>"
    ]
})

model.resize_token_embeddings(len(tokenizer))
model.to(DEVICE)


def build_prompt(instruction):
    conv = conv_templates["vicuna_v1"].copy()
    conv.append_message(
        conv.roles[0],
        f"<image>\n{instruction}"
    )
    conv.append_message(conv.roles[1], None)
    return conv.get_prompt()




# Unified step (train or infer)

def cot_vla_step(
    instruction,
    image_tensor,
    target=None,
    optimizer=None
):
    prompt = build_prompt(instruction)

    # ---------- INFERENCE ----------
    if target is None:
        input_ids = tokenizer_image_token(
            prompt,
            tokenizer,
            IMAGE_TOKEN_INDEX,
            return_tensors="pt"
        ).to(model.device)

        with torch.inference_mode():
            output_ids = model.generate(
                input_ids,
                images=image_tensor,
                max_new_tokens=256,
                do_sample=False
            )

        text = tokenizer.decode(output_ids[0], skip_special_tokens=False)
        text = re.sub(r"<cot>[\\s\\S]*?</cot>", "", text)
        return text.strip()

    # ---------- TRAINING ----------
    cot, action = target

    target_text = (
        "<cot>\n"
        f"{cot}\n"
        "</cot>\n"
        "<action>\n"
        f"{action}\n"
        "</action>"
    )

    full_text = prompt + target_text

    input_ids = tokenizer_image_token(
        full_text,
        tokenizer,
        IMAGE_TOKEN_INDEX,
        return_tensors="pt"
    ).to(model.device)

    labels = input_ids.clone()
    labels[:] = -100

    cot_id = tokenizer.convert_tokens_to_ids("<cot>")
    cot_pos = (input_ids == cot_id).nonzero(as_tuple=True)[1]

    if len(cot_pos) > 0:
        labels[:, cot_pos[0]:] = input_ids[:, cot_pos[0]:]

    outputs = model(
        input_ids=input_ids,
        labels=labels,
        images=image_tensor
    )

    loss = outputs.loss
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()

    return loss.item()

############################################
# Training loop
############################################

def train(json_path, image_tensor):
    dataset = CoTVLADataset(json_path)
    loader = DataLoader(dataset, batch_size=1, shuffle=True)

    optimizer = AdamW(model.parameters(), lr=LR)
    model.train()

    for epoch in range(EPOCHS):
        for step, batch in enumerate(loader):
            loss = cot_vla_step(
                instruction=None,  # ignored
                image_tensor=image_tensor,
                target=(
                    "\n".join(dataset.samples[step]["visual_cot"]),
                    "\n".join(dataset.samples[step]["action"])
                ),
                optimizer=optimizer
            )

            print(f"Epoch {epoch} | Step {step} | Loss {loss:.4f}")

############################################
# Example usage
############################################

if __name__ == "__main__":
     image_tensor = process_images([image], image_processor, model.config).to(model.device, dtype=model.dtype)

     train("train.json", image_tensor)

    # inference
    # print(cot_vla_step(
    #     instruction="Pick up the red cup",
    #     image_tensor=image_tensor
    # ))
    pass